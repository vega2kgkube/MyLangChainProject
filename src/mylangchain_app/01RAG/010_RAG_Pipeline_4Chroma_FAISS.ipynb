{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1F5lTDp5UPf0",
   "metadata": {
    "id": "1F5lTDp5UPf0"
   },
   "source": [
    "### 1) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6",
   "metadata": {
    "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6"
   },
   "outputs": [],
   "source": [
    "#poetry add langchain_community chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55152049-e9e5-4952-8e19-409f58cf3ac9",
   "metadata": {
    "id": "55152049-e9e5-4952-8e19-409f58cf3ac9"
   },
   "source": [
    "### 2) OpenAI ì¸ì¦í‚¤ ì„¤ì •\n",
    "https://openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f68a8-4745-4377-8057-6090b87377d1",
   "metadata": {
    "id": "b76f68a8-4745-4377-8057-6090b87377d1"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9abc5",
   "metadata": {},
   "source": [
    "##### Chroma ê°„ë‹¨í•œ ì˜ˆì œ\n",
    "* Chroma DBì— í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f0430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings  # OpenAI ì„ë² ë”© ì‚¬ìš©\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
    "from langchain_chroma import Chroma  # ë²¡í„° DB (Chroma) ì‚¬ìš©\n",
    "\n",
    "\n",
    "# 2. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "DB_PATH = \"./db/chroma_db\"\n",
    "\n",
    "# 3. í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¬¸ì„œë¥¼ ë¶„í• í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "def load_and_split_text(file_path, splitter):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•œ í›„, ì„¤ì •ëœ Splitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ\n",
    "        splitter (RecursiveCharacterTextSplitter): í…ìŠ¤íŠ¸ ë¶„í• ê¸° ê°ì²´\n",
    "\n",
    "    Returns:\n",
    "        list: ë¶„í• ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        loader = TextLoader(file_path)  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ\n",
    "        return loader.load_and_split(splitter)  # ë¶„í• í•˜ì—¬ ë°˜í™˜\n",
    "    except Exception as e:\n",
    "        print(f\" íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}\")\n",
    "        return []\n",
    "\n",
    "# 4. í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • (600ì ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , 100ì ê²¹ì¹¨ í¬í•¨)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "\n",
    "# 5. ë‘ ê°œì˜ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ë° ë¶„í• \n",
    "split_doc1 = load_and_split_text(\"data/ai-terminology.txt\", text_splitter)\n",
    "split_doc2 = load_and_split_text(\"data/finance-terminology.txt\", text_splitter)\n",
    "\n",
    "# 6. ë¬¸ì„œ ê°œìˆ˜ ì¶œë ¥\n",
    "print(f\"AI ë¬¸ì„œ ê°œìˆ˜: {len(split_doc1)}\")\n",
    "print(f\"ê¸ˆìœµ ë¬¸ì„œ ê°œìˆ˜: {len(split_doc2)}\")\n",
    "\n",
    "# 7. ëª¨ë“  ë¬¸ì„œ í•©ì¹˜ê¸°\n",
    "all_documents = split_doc1 + split_doc2\n",
    "\n",
    "# 8. Chroma ë²¡í„° DB ìƒì„± ë° ì €ì¥\n",
    "try:\n",
    "    persist_db = Chroma.from_documents(\n",
    "        documents=all_documents,\n",
    "        embedding=OpenAIEmbeddings(),  # OpenAI Embeddings ì‚¬ìš©\n",
    "        persist_directory=DB_PATH,  # ë²¡í„° DB ì €ì¥ ìœ„ì¹˜ ì§€ì •\n",
    "        collection_name=\"my_vector_db\",  # ë°ì´í„°ë² ì´ìŠ¤ ì»¬ë ‰ì…˜ ì´ë¦„\n",
    "    )\n",
    "    print(\"Chroma ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\" Chroma ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 9. ì €ì¥ëœ ë°ì´í„° í™•ì¸\n",
    "try:\n",
    "    retrieved_docs = persist_db.get()  # Chroma DBì—ì„œ ë°ì´í„° ì¡°íšŒ\n",
    "    print(f\" ì €ì¥ëœ ë²¡í„° ê°œìˆ˜: {len(retrieved_docs['ids'])}, íƒ€ì… {type(retrieved_docs['ids'])}\")\n",
    "except Exception as e:\n",
    "    print(f\" ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 10. ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜\n",
    "def search_query(query, k=2):\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ì ì…ë ¥(query)ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    Args:\n",
    "        query (str): ê²€ìƒ‰í•  ë¬¸ì¥ (ì˜ˆ: \"Transformer ê°œë… ì„¤ëª…\")\n",
    "        k (int, optional): ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        None: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = persist_db.similarity_search(query, k=k)  # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "        print(f\"\\n [Query]: {query}\\n\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"ğŸ”¹ [Result {i+1}]: {doc.page_content[:300]}...\\n\")  # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "    except Exception as e:\n",
    "        print(f\" ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 11. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "search_query(\"Transformer ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\", k=2)\n",
    "search_query(\"Hedge Fund ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜?\", k=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fbd63b",
   "metadata": {},
   "source": [
    "##### FAISS ê°„ë‹¨í•œ ì˜ˆì œ\n",
    "* FAISS DBì— í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ba88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
    "from langchain_community.vectorstores import FAISS  # ë²¡í„° DB (FAISS) ì‚¬ìš©\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# 2. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "DB_PATH = \"../db/faiss_db\"\n",
    "\n",
    "# 3. í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¬¸ì„œë¥¼ ë¶„í• í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "def load_and_split_text(file_path, splitter):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•œ í›„, ì„¤ì •ëœ Splitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ\n",
    "        splitter (RecursiveCharacterTextSplitter): í…ìŠ¤íŠ¸ ë¶„í• ê¸° ê°ì²´\n",
    "\n",
    "    Returns:\n",
    "        list: ë¶„í• ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ\n",
    "        return loader.load_and_split(splitter)  # ë¶„í• í•˜ì—¬ ë°˜í™˜\n",
    "    except Exception as e:\n",
    "        print(f\" íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}\")\n",
    "        return []\n",
    "\n",
    "# 4. í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • (600ì ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , 100ì ê²¹ì¹¨ í¬í•¨)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "\n",
    "# 5. ë‘ ê°œì˜ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ë° ë¶„í• \n",
    "split_doc1 = load_and_split_text(\"../data/ai-terminology.txt\", text_splitter)\n",
    "split_doc2 = load_and_split_text(\"../data/finance-terminology.txt\", text_splitter)\n",
    "\n",
    "# 6. ë¬¸ì„œ ê°œìˆ˜ ì¶œë ¥\n",
    "print(f\"AI ë¬¸ì„œ ê°œìˆ˜: {len(split_doc1)}\")\n",
    "print(f\"ê¸ˆìœµ ë¬¸ì„œ ê°œìˆ˜: {len(split_doc2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c371fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. ëª¨ë“  ë¬¸ì„œ í•©ì¹˜ê¸°\n",
    "all_documents = split_doc1 + split_doc2\n",
    "\n",
    "# 8. FAISS ë²¡í„° DB ìƒì„± ë° ì €ì¥\n",
    "try:\n",
    "    ollamaEmbeddings = OllamaEmbeddings(model=\"bge-m3:latest\")\n",
    "\n",
    "    # FAISS ë²¡í„° DB ìƒì„±\n",
    "    persist_db = FAISS.from_documents(\n",
    "        documents=all_documents,\n",
    "        embedding=ollamaEmbeddings,\n",
    "    )\n",
    "    \n",
    "    # ë¡œì»¬ ë””ìŠ¤í¬ì— ì €ì¥\n",
    "    if not os.path.exists(DB_PATH):\n",
    "        os.makedirs(DB_PATH)\n",
    "    persist_db.save_local(DB_PATH)\n",
    "    \n",
    "    print(\"FAISS ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\" FAISS ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 9. ì €ì¥ëœ ë°ì´í„° í™•ì¸\n",
    "try:\n",
    "    # FAISSì—ì„œ ì¸ë±ìŠ¤ ì •ë³´ í™•ì¸\n",
    "    print(f\" ì €ì¥ëœ ë²¡í„° ê°œìˆ˜: {persist_db.index.ntotal}\")\n",
    "    print(f\" ë²¡í„° ì°¨ì›: {persist_db.index.d}\")\n",
    "    print(f\" ì¸ë±ìŠ¤ íƒ€ì…: {type(persist_db.index)}\")\n",
    "except Exception as e:\n",
    "    print(f\" ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10. ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜\n",
    "def search_query(query, k=5):\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ì ì…ë ¥(query)ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    Args:\n",
    "        query (str): ê²€ìƒ‰í•  ë¬¸ì¥ (ì˜ˆ: \"Transformer ê°œë… ì„¤ëª…\")\n",
    "        k (int, optional): ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        None: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = persist_db.similarity_search(query, k=k)  # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "        print(f\"\\n [Query]: {query}\\n\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"[Result {i+1}]: {doc.page_content[:300]}...\\n\")  # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "    except Exception as e:\n",
    "        print(f\" ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 11. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "print('AI ìš©ì–´ =================')\n",
    "search_query(\"Embedding ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\", k=3)\n",
    "print('Finance ìš©ì–´ =================')\n",
    "search_query(\"Hedge Fund ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\", k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4585881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 12. ì €ì¥ëœ FAISS DB ë¡œë“œ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ì €ì¥ëœ FAISS DB ë¡œë“œ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    ollamaEmbeddings = OllamaEmbeddings(model=\"bge-m3:latest\")\n",
    "    # ì €ì¥ëœ FAISS DB ë¡œë“œ\n",
    "    loaded_db = FAISS.load_local(\n",
    "        DB_PATH, \n",
    "        ollamaEmbeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    print(\"FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ë¡œë“œëœ DBë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "    print(f\"ë¡œë“œëœ ë²¡í„° ê°œìˆ˜: {loaded_db.index.ntotal}\")\n",
    "    \n",
    "    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "    test_results = loaded_db.similarity_search(\"í•™ìŠµì—ëŠ” ì–´ë–¤ê²ƒë“¤ì´ ìˆë‚˜ìš”?\", k=3)\n",
    "    print(f\"\\në¡œë“œëœ DB ê²€ìƒ‰ ê²°ê³¼: {test_results[0].page_content[:300]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"FAISS DB ë¡œë“œ ì˜¤ë¥˜: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e78988a",
   "metadata": {},
   "source": [
    "* [k-ìµœê·¼ì ‘ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜](https://ko.wikipedia.org/wiki/K-%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)\n",
    "* FAISSì—ì„œ similarity_search_with_score() í•¨ìˆ˜ì˜ ì ìˆ˜ëŠ” ì‹¤ì œë¡œëŠ” **ê±°ë¦¬(distance)**ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "    * ë‚®ì€ ì ìˆ˜ = ë” ìœ ì‚¬í•¨ (ê±°ë¦¬ê°€ ê°€ê¹Œì›€)\n",
    "    : Result 1 (Score: 0.3795) â†’ ë” ìœ ì‚¬í•œ ê²°ê³¼\n",
    "    * ë†’ì€ ì ìˆ˜ = ëœ ìœ ì‚¬í•¨ (ê±°ë¦¬ê°€ ë©€ìŒ)\n",
    "    : Result 2 (Score: 0.4341) â†’ ëœ ìœ ì‚¬í•œ ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13183274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 13. ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def search_with_score(query, k=5):\n",
    "    \"\"\"\n",
    "    ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = persist_db.similarity_search_with_score(query, k=k)\n",
    "        print(f\"\\n [Query]: {query}\\n\")\n",
    "        for i, (doc, score) in enumerate(results):\n",
    "            print(f\"[Result {i+1}] (Score: {score:.4f}):\")\n",
    "            print(f\"{doc.page_content[:500]}...\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\" ì ìˆ˜ ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "search_with_score(\"AI ì¢…ë¥˜ëŠ” ì–´ë–¤ê²ƒë“¤ì´ ìˆë‚˜ìš”?\", k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4fa42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 14. FAISS ì¸ë±ìŠ¤ ì •ë³´ ì¶œë ¥\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FAISS ì¸ë±ìŠ¤ ìƒì„¸ ì •ë³´\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    index = persist_db.index\n",
    "    print(f\"ì¸ë±ìŠ¤ íƒ€ì…: {type(index).__name__}\")\n",
    "    print(f\"ì´ ë²¡í„° ìˆ˜: {index.ntotal}\")\n",
    "    print(f\"ë²¡í„° ì°¨ì›: {index.d}\")\n",
    "    print(f\"í›ˆë ¨ ì—¬ë¶€: {index.is_trained}\")\n",
    "    print(f\"ë©”íŠ¸ë¦­ íƒ€ì…: {index.metric_type}\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì¶”ì •)\n",
    "    memory_usage = index.ntotal * index.d * 4 / (1024 * 1024)  # 4 bytes per float, MB ë‹¨ìœ„\n",
    "    print(f\"ì¶”ì • ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d910ef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "chatbot-0lCeHk3W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
