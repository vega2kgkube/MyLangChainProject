{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry add pypdf=\">=4.2.0,<5.0.0\"\n",
    "# poetry add langchain-upstage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "- `.env` íŒŒì¼ì— `UPSTAGE_API_KEY` ë“±ë¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up_v\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv()\n",
    "\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "print(UPSTAGE_API_KEY[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LLM ë‹µë³€ ìƒì„±\n",
    "\n",
    "- Upstage Consoleì—ì„œ ë°œê¸‰ë°›ì€ API Keyë¥¼ `UPSTAGE_API_KEY`ë¼ê³  ì €ì¥í•˜ë©´ ë³„ë„ì˜ ì„¤ì • ì—†ì´ `ChatUpstage`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000022A66062840> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000022A6607AF00> model_name='solar-pro' temperature=0.5 model_kwargs={} upstage_api_key=SecretStr('**********') upstage_api_base='https://api.upstage.ai/v1'\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "#llm = ChatUpstage(temperature=0.5)\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "**LangChain**ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ê°„ì†Œí™”í•˜ê¸° ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. 2022ë…„ Harrison Chaseê°€ ì£¼ë„í•˜ì—¬ ê°œë°œë˜ì—ˆìœ¼ë©°, ë³µì¡í•œ LLM ê¸°ë°˜ ì‹œìŠ¤í…œì„ ëª¨ë“ˆì‹ìœ¼ë¡œ êµ¬ì„±í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤. ì£¼ìš” íŠ¹ì§•ê³¼ í™œìš© ë¶„ì•¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "### 1. **í•µì‹¬ ê¸°ëŠ¥**\n",
      "   - **ëª¨ë“ˆí™”**: LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ êµ¬ì„± ìš”ì†Œ(ë°ì´í„° ë¡œë“œ, ì²´ì¸, ë©”ëª¨ë¦¬, ì—ì´ì „íŠ¸ ë“±)ë¥¼ ë ˆê³  ì¡°ë¦½ì²˜ëŸ¼ ì¡°í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "   - **í†µí•© ì§€ì›**: OpenAI, Anthropic, Hugging Face, ë¡œì»¬ LLM ë“± ë‹¤ì–‘í•œ ëª¨ë¸ ë° ë„êµ¬ì™€ ì—°ë™ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "   - **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ëŒ€í™” ê¸°ë¡ì´ë‚˜ ìƒíƒœ ìœ ì§€ë¥¼ í†µí•´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ëŠ” ì±—ë´‡ êµ¬ì¶•ì— ìœ ìš©í•©ë‹ˆë‹¤.\n",
      "   - **ì—ì´ì „íŠ¸ ê¸°ëŠ¥**: LLMì´ ì™¸ë¶€ ë„êµ¬(ê²€ìƒ‰, ê³„ì‚°ê¸°, ë°ì´í„°ë² ì´ìŠ¤ ë“±)ë¥¼ ììœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
      "\n",
      "### 2. **ì£¼ìš” êµ¬ì„± ìš”ì†Œ**\n",
      "   - **ëª¨ë¸ I/O**: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿, ì¶œë ¥ íŒŒì„œ ë“±ìœ¼ë¡œ LLMê³¼ì˜ ì…ë ¥ì„ í‘œì¤€í™”í•©ë‹ˆë‹¤.\n",
      "   - **ì²´ì¸(Chains)**: ì—¬ëŸ¬ LLM í˜¸ì¶œê³¼ ë„êµ¬ë¥¼ ì—°ê²°í•´ ë³µí•© ì‘ì—…(ì˜ˆ: ë¬¸ì„œ QA â†’ ìš”ì•½ â†’ ë‹µë³€)ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
      "   - **ë°ì´í„° ì—°ê²°**: API, ë°ì´í„°ë² ì´ìŠ¤, PDF ë“± ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤ì™€ ì—°ë™í•©ë‹ˆë‹¤.\n",
      "   - **ì½œë°± ì‹œìŠ¤í…œ**: ì§„í–‰ ìƒí™© ì¶”ì ì´ë‚˜ ë¡œê¹…ì„ ìœ„í•œ í›„í‚¹ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "### 3. **ì‚¬ìš© ì‚¬ë¡€**\n",
      "   - **ì±—ë´‡**: ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ëŠ” ëŒ€í™”í˜• AI.\n",
      "   - **ìë™í™”ëœ ë¬¸ì„œ ì²˜ë¦¬**: ë³´ê³ ì„œ ìš”ì•½, ë°ì´í„° ì¶”ì¶œ ë“±.\n",
      "   - **ì½”ë“œ ìƒì„±/ë¦¬íŒ©í† ë§**: GitHub Copilotê³¼ ìœ ì‚¬í•œ ë„êµ¬ êµ¬ì¶•.\n",
      "   - **ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ**: ê²€ìƒ‰, ê³„ì‚°, ì˜ì‚¬ ê²°ì •ì„ ìë™í™”í•˜ëŠ” ììœ¨ ì—ì´ì „íŠ¸.\n",
      "\n",
      "### 4. **ì¥ì **\n",
      "   - **ê°œë°œ íš¨ìœ¨ì„±**: ë°˜ë³µì ì¸ ì½”ë“œ ì‘ì„± ì—†ì´ ë¹ ë¥´ê²Œ í”„ë¡œí† íƒ€ì´í•‘ ê°€ëŠ¥.\n",
      "   - **í™•ì¥ì„±**: ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì¶”ê°€ê°€ ìš©ì´í•©ë‹ˆë‹¤.\n",
      "   - **ì»¤ë®¤ë‹ˆí‹°**: í™œë°œí•œ ìƒíƒœê³„ë¡œ ë‹¤ì–‘í•œ ì˜ˆì œì™€ í†µí•© ë„êµ¬ê°€ ê³µìœ ë©ë‹ˆë‹¤.\n",
      "\n",
      "### 5. **ì˜ˆì‹œ ì½”ë“œ (ê°„ë‹¨í•œ ì²´ì¸)**\n",
      "```python\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.prompts import PromptTemplate\n",
      "\n",
      "prompt = PromptTemplate.from_template(\"ì§ˆë¬¸: {question} â†’ ë‹µë³€: \")\n",
      "llm = OpenAI(model_name=\"gpt-3.5-turbo\")\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "chain.run(\"íƒœì–‘ì˜ ì§ˆëŸ‰ì€ ì§€êµ¬ì˜ ëª‡ ë°°ì¸ê°€ìš”?\")\n",
      "```\n",
      "\n",
      "LangChainì€ LLMì˜ ì ì¬ë ¥ì„ ìµœëŒ€í•œ í™œìš©í•˜ë ¤ëŠ” ê°œë°œì, ì—°êµ¬ì, ê¸°ì—…ì—ê²Œ ê°•ë ¥í•œ ë„êµ¬ë¡œ ìë¦¬ì¡ê³  ìˆìŠµë‹ˆë‹¤. ê³µì‹ ë¬¸ì„œ([langchain.com](https://python.langchain.com))ì—ì„œ ë” ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "ai_message=llm.invoke(\"LangChainì€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
    "print(type(ai_message))\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangChain**ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM, Large Language Model)ì„ í™œìš©í•´ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ **ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬**ì…ë‹ˆë‹¤. ë³µì¡í•œ ì–¸ì–´ ëª¨ë¸ ì‘ì—…ì„ ë‹¨ìˆœí™”í•˜ê³ , ì™¸ë¶€ ë„êµ¬ì™€ì˜ í†µí•©ì„ ì‰½ê²Œ í•´ì£¼ëŠ” ê²ƒì´ í•µì‹¬ ëª©í‘œì…ë‹ˆë‹¤.  \n",
      "\n",
      "### ğŸ“Œ **ì£¼ìš” ê°œë… ë° íŠ¹ì§•**\n",
      "1. **ëª¨ë“ˆí™”ëœ êµ¬ì„± ìš”ì†Œ**  \n",
      "   - **LLM ì—°ê²°**: OpenAI, Anthropic, Meta ë“±ì˜ ë‹¤ì–‘í•œ LLMê³¼ í˜¸í™˜ë©ë‹ˆë‹¤.  \n",
      "   - **ë©”ëª¨ë¦¬(Memory)**: ëŒ€í™” ê¸°ë¡ì„ ìœ ì§€í•´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤ (ì˜ˆ: ì±„íŒ…ë´‡).  \n",
      "   - **ì²´ì¸(Chain)**: ì—¬ëŸ¬ ë‹¨ê³„(ì˜ˆ: \"ì§ˆë¬¸ ë¶„ì„ â†’ ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ â†’ ë‹µë³€ ìƒì„±\")ë¥¼ ì—°ê²°í•´ ë³µì¡í•œ ì‘ì—…ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.  \n",
      "   - **ì—ì´ì „íŠ¸(Agent)**: ë™ì ìœ¼ë¡œ ë„êµ¬(ê²€ìƒ‰, ê³„ì‚°ê¸° ë“±)ë¥¼ ì„ íƒí•´ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.  \n",
      "   - **ë°ì´í„° ì—°ê²°**: SQL, ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤(FAISS, Pinecone), API ë“±ê³¼ í†µí•©í•´ ì™¸ë¶€ ë°ì´í„°ë¥¼ í™œìš©í•©ë‹ˆë‹¤.  \n",
      "\n",
      "2. **ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€**  \n",
      "   - **ì±—ë´‡**: ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ë©° ë‹µë³€.  \n",
      "   - **ë¬¸ì„œ ìš”ì•½/ë¶„ì„**: PDF, ì›¹ í˜ì´ì§€ ë“±ì—ì„œ ì •ë³´ ì¶”ì¶œ.  \n",
      "   - **ì½”ë“œ ìƒì„±**: GitHub ë¦¬í¬ì§€í† ë¦¬ì™€ ì—°ë™í•´ ê°œë°œ ì§€ì›.  \n",
      "   - **ìë™í™”ëœ ì›Œí¬í”Œë¡œìš°**: ì—¬ëŸ¬ APIì™€ ë„êµ¬ë¥¼ ì¡°í•©í•´ ë³µì¡í•œ ì‘ì—… ì²˜ë¦¬.  \n",
      "\n",
      "3. **ì¥ì **  \n",
      "   - LLMì˜ í•œê³„ë¥¼ ë³´ì™„í•˜ëŠ” **ë„êµ¬ í†µí•©** (ì˜ˆ: ì‹¤ì‹œê°„ ì •ë³´ ê²€ìƒ‰ì„ ìœ„í•´ SerpAPI ì‚¬ìš©).  \n",
      "   - ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸ë¡œ **ê°œë°œ íš¨ìœ¨ì„±** í–¥ìƒ.  \n",
      "   - ì»¤ë®¤ë‹ˆí‹°ê°€ í™œë°œíˆ ê¸°ì—¬í•´ **í™•ì¥ì„±**ì´ ë›°ì–´ë‚¨.  \n",
      "\n",
      "4. **ì˜ˆì‹œ ì½”ë“œ (ê°„ë‹¨í•œ ì²´ì¸)**  \n",
      "   ```python\n",
      "   from langchain.chains import LLMChain\n",
      "   from langchain.llms import OpenAI\n",
      "   from langchain.prompts import PromptTemplate\n",
      "\n",
      "   llm = OpenAI(model_name=\"gpt-3.5-turbo\")\n",
      "   prompt = PromptTemplate(input_variables=[\"topic\"], template=\"ë‹¤ìŒì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜: {topic}\")\n",
      "   chain = LLMChain(llm=llm, prompt=prompt)\n",
      "   chain.run(\"ì–‘ì ì»´í“¨íŒ…\")\n",
      "   ```\n",
      "\n",
      "### ğŸ” **LangSmith (ëª¨ë‹ˆí„°ë§ ë„êµ¬)**  \n",
      "LangChain íŒ€ì€ **LangSmith**ë¼ëŠ” ë””ë²„ê¹…/ëª¨ë‹ˆí„°ë§ í”Œë«í¼ë„ ì œê³µí•©ë‹ˆë‹¤. ì²´ì¸ì˜ ì„±ëŠ¥ ë¶„ì„ì´ë‚˜ ì˜¤ë¥˜ ì¶”ì ì— ìœ ìš©í•©ë‹ˆë‹¤.  \n",
      "\n",
      "LangChainì€ LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ **ë ˆê³  ì¡°ë¦½**ì²˜ëŸ¼ ì‰½ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” í”„ë ˆì„ì›Œí¬ë¡œ, ë³µì¡í•œ AI ì‘ì—…ì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ğŸš€"
     ]
    }
   ],
   "source": [
    "# using chat stream\n",
    "for chunk in llm.stream(\"LangChainì€ ë¬´ì—‡ì¸ê°€ìš”?\"):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upstage Response:\n",
      "\"LangChainì€ AI ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•ì„ ìœ„í•œ ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\"  \n",
      "\n",
      "ë˜ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì€ ë²„ì „:  \n",
      "**\"LangChainì€ AI ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì— ìµœì í™”ëœ ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\"**  \n",
      "\n",
      "### ì„¤ëª…:\n",
      "- **\"ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬\"** â†’ \"powerful framework\"ì˜ ì§ì—­ìœ¼ë¡œ, ê¸°ìˆ  ë¬¸ì„œì—ì„œ í”íˆ ì‚¬ìš©ë˜ëŠ” í‘œí˜„ì…ë‹ˆë‹¤.  \n",
      "- **\"êµ¬ì¶•\"** ëŒ€ì‹  **\"ê°œë°œì— ìµœì í™”ëœ\"**ì„ ì¶”ê°€í•˜ë©´ í•œêµ­ì–´ë¡œ ë” ë§¤ë„ëŸ¬ìš´ ë¬¸ì¥ì´ ë©ë‹ˆë‹¤. (ì„ íƒ ì‚¬í•­)  \n",
      "- ê¸°ìˆ  ìš©ì–´ì¸ **\"AI ì• í”Œë¦¬ì¼€ì´ì…˜\"**ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ì§€ë§Œ, í•„ìš”ì— ë”°ë¼ **\"ì¸ê³µì§€ëŠ¥ ì‘ìš© í”„ë¡œê·¸ë¨\"**ìœ¼ë¡œ ë°”ê¿€ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "ë¬¸ë§¥ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ ì„ íƒí•˜ì‹œë©´ ë©ë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "translation_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a professional translator specializing in Korean-English translation.\"),\n",
    "        (\"human\", \"Translate this from {source_lang} to {target_lang}: {text}\")\n",
    "    ])\n",
    "\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰\n",
    "chain = translation_prompt | llm\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"source_lang\": \"English\",\n",
    "    \"target_lang\": \"Korean\", \n",
    "    \"text\": \"LangChain is a powerful framework for building AI applications.\"\n",
    "})\n",
    "\n",
    "print(\"Upstage Response:\")\n",
    "print(response.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "# using chain\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that translates English to Korean.\"),\n",
    "        (\"human\", \"Translate this sentence from English to Korean. {english_text}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatUpstage()\n",
    "chain = prompt | llm\n",
    "\n",
    "ai_message=chain.invoke({\"english_text\": \"Hello, How are you?\"})\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groundness Check\n",
    "* Groundedness Check APIëŠ” ì‚¬ìš©ìê°€ ì œê³µí•œ Context(ì»¨í…ìŠ¤íŠ¸)ì— ëŒ€í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‘ë‹µì´ ì‹¤ì œë¡œ ê·¸ ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ê³  ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageGroundednessCheck\n",
    "\n",
    "groundedness_check = UpstageGroundednessCheck()\n",
    "\n",
    "request_input = {\n",
    "    \"context\": \"ì‚¼ì„±ì „ìëŠ” ì—°ê²° ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¶œ 74.07ì¡°ì›, ì˜ì—…ì´ìµ 10.44ì¡°ì›ì˜ 2024ë…„ 2ë¶„ê¸° ì‹¤ì ì„ ë°œí‘œí–ˆë‹¤. ì „ì‚¬ ë§¤ì¶œì€ ì „ë¶„ê¸° ëŒ€ë¹„ 3% ì¦ê°€í•œ 74.07ì¡°ì›ì„ ê¸°ë¡í–ˆë‹¤. DSë¶€ë¬¸ì€ ë©”ëª¨ë¦¬ ì—…í™© íšŒë³µìœ¼ë¡œ ì „ë¶„ê¸° ëŒ€ë¹„ 23% ì¦ê°€í•˜ê³ , SDCëŠ” OLED íŒë§¤ í˜¸ì¡°ë¡œ ì¦ê°€í–ˆë‹¤.\",\n",
    "    \"answer\": \"ì‚¼ì„±SDSì˜ 2024ë…„ 2ë¶„ê¸° ë§¤ì¶œì€ ì•½ 74.07ì¡°ì›ì´ë‹¤.\",\n",
    "}\n",
    "\n",
    "response = groundedness_check.invoke(request_input)\n",
    "print(response)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
