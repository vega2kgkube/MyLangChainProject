{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello LangChain!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsk_n\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "#load_dotenv(dotenv_path='.env')\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt + llm + output \n",
    "\n",
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.\") , \n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "System: ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.\n",
      "Human: íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”? ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”\n"
     ]
    }
   ],
   "source": [
    "prompt_text = prompt.format(input=\"íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”? ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”\")\n",
    "print(type(prompt_text))\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_openai.chat_models.base.ChatOpenAI'>\n",
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000002669E0B9E50> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002669E0BB6E0> root_client=<openai.OpenAI object at 0x000002669E0BB500> root_async_client=<openai.AsyncOpenAI object at 0x000002669E0BB560> model_name='openai/gpt-oss-120b' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "#llm = ChatOpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# Groq APIë¥¼ ì‚¬ìš©í•˜ëŠ” ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    #model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(type(llm))\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(\"ì‘ë‹µ:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "first=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002669E0B9E50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002669E0BB6E0>, root_client=<openai.OpenAI object at 0x000002669E0BB500>, root_async_client=<openai.AsyncOpenAI object at 0x000002669E0BB560>, model_name='openai/gpt-oss-120b', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "print(type(chain))\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\":\"LangChainì€ ë¬´ì—‡ì¸ê°€ìš”?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "## LangChainì´ë€?\n",
      "\n",
      "**LangChain**ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM, Large Language Model)ì„ **ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜ì¤€**ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆê²Œ í•´ ì£¼ëŠ” **ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬**ì…ë‹ˆë‹¤.  \n",
      "LLMì„ ë‹¨ìˆœíˆ â€œí…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì—”ì§„â€ìœ¼ë¡œ ì“°ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ë°ì´í„°ë² ì´ìŠ¤, API, íŒŒì¼ ì‹œìŠ¤í…œ, ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ë“± ë‹¤ì–‘í•œ ì™¸ë¶€ ìì›**ê³¼ ê²°í•©í•´ **ë³µí•©ì ì¸ ì›Œí¬í”Œë¡œìš°**ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## í•µì‹¬ ê°œë…\n",
      "\n",
      "| ê°œë… | ì„¤ëª… | ì˜ˆì‹œ |\n",
      "|------|------|------|\n",
      "| **Chain** | ì—¬ëŸ¬ LLM í˜¸ì¶œÂ·í”„ë¡¬í”„íŠ¸Â·ì²˜ë¦¬ ë‹¨ê³„ë¥¼ **ìˆœì°¨Â·ì¡°ê±´ë¶€Â·ë°˜ë³µ**ìœ¼ë¡œ ì—°ê²°í•œ íë¦„ | ì§ˆë¬¸ â†’ ê²€ìƒ‰ â†’ ìš”ì•½ â†’ ë‹µë³€ |\n",
      "| **PromptTemplate** | ë³€ìˆ˜(`{question}` ë“±)ë¥¼ ì‚½ì…í•´ ë™ì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„± | `\"ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ ì£¼ì„¸ìš”: {question}\"` |\n",
      "| **LLM** | ì‹¤ì œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸ (OpenAI GPTâ€‘4, Anthropic Claude, Llama ë“±) | `OpenAIChat`, `ChatAnthropic` |\n",
      "| **Memory** | ëŒ€í™”Â·ì‘ì—… ì´ë ¥ì„ **ìƒíƒœ**ë¡œ ì €ì¥í•´ ë‹¤ìŒ í˜¸ì¶œì— í™œìš© | ëŒ€í™” ê¸°ë¡, ì‘ì—… ì§„í–‰ ìƒí™© |\n",
      "| **Agents** | **íˆ´(tool)**(ê²€ìƒ‰ ì—”ì§„, ë°ì´í„°ë² ì´ìŠ¤, ì½”ë“œ ì‹¤í–‰ ë“±)ì„ **ë™ì ìœ¼ë¡œ ì„ íƒ**í•´ ì‘ì—… ìˆ˜í–‰ | â€œë‚ ì”¨ë¥¼ ì•Œë ¤ì¤˜â€ â†’ ê²€ìƒ‰ íˆ´ í˜¸ì¶œ â†’ ê²°ê³¼ ë°˜í™˜ |\n",
      "| **Tools** | ì™¸ë¶€ ì„œë¹„ìŠ¤Â·í•¨ìˆ˜Â·APIë¥¼ ë˜í•‘í•œ **í”ŒëŸ¬ê·¸ì¸** | `SerpAPI`, `SQLDatabase`, `PythonREPL` |\n",
      "| **Retriever** | í…ìŠ¤íŠ¸/ë¬¸ì„œ ì»¬ë ‰ì…˜ì—ì„œ **ê´€ë ¨ ë¬¸ì„œ**ë¥¼ ê²€ìƒ‰í•´ LLMì— ì œê³µ | ë²¡í„° DB(FAISS, Pinecone) ê¸°ë°˜ ê²€ìƒ‰ |\n",
      "| **Callbacks** | ì‹¤í–‰ íë¦„ì„ **ê°ì‹œÂ·ë¡œê¹…Â·ì‹œê°í™”**í•  ìˆ˜ ìˆëŠ” í›… | LangSmith(ëª¨ë‹ˆí„°ë§), Streamlit UI ì—°ë™ |\n",
      "\n",
      "---\n",
      "\n",
      "## ì£¼ìš” ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤\n",
      "\n",
      "| ì‹œë‚˜ë¦¬ì˜¤ | êµ¬í˜„ ë°©ë²• (LangChain) |\n",
      "|----------|----------------------|\n",
      "| **ì§ˆì˜ì‘ë‹µ(Q&A) ì±—ë´‡** | `RetrievalQAChain` â†’ ë²¡í„° DBì—ì„œ ë¬¸ì„œ ê²€ìƒ‰ â†’ LLMì´ ë‹µë³€ |\n",
      "| **ì—ì´ì „íŠ¸ ê¸°ë°˜ ìë™í™”** | `AgentExecutor` + ì—¬ëŸ¬ `Tool` (ê²€ìƒ‰, ì½”ë“œ ì‹¤í–‰, DB ì¿¼ë¦¬) |\n",
      "| **ë¬¸ì„œ ìš”ì•½Â·ë¶„ì„** | `MapReduceDocumentsChain` ë˜ëŠ” `RefineDocumentsChain` |\n",
      "| **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§** | `PromptTemplate` + `LLMChain` ë¡œ ë°˜ë³µ í…ŒìŠ¤íŠ¸ |\n",
      "| **ë©€í‹°ìŠ¤í… ì›Œí¬í”Œë¡œìš°** | `SequentialChain` í˜¹ì€ `GraphChain` ìœ¼ë¡œ ë³µí•© ë¡œì§ êµ¬í˜„ |\n",
      "| **ëŒ€í™” ë©”ëª¨ë¦¬** | `ConversationBufferMemory`, `ConversationSummaryMemory` ë“± ì‚¬ìš© |\n",
      "\n",
      "---\n",
      "\n",
      "## ê¸°ë³¸ ì½”ë“œ ì˜ˆì‹œ (Python)\n",
      "\n",
      "```python\n",
      "from langchain import OpenAI, PromptTemplate, LLMChain\n",
      "from langchain.chains import RetrievalQAChain\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.document_loaders import TextLoader\n",
      "\n",
      "# 1ï¸âƒ£ ë¬¸ì„œ ë¡œë“œ & ì„ë² ë”© â†’ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•\n",
      "loader = TextLoader(\"data/faq.txt\")\n",
      "docs = loader.load()\n",
      "embeddings = OpenAIEmbeddings()\n",
      "vectorstore = FAISS.from_documents(docs, embeddings)\n",
      "\n",
      "# 2ï¸âƒ£ Retriever ìƒì„±\n",
      "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
      "\n",
      "# 3ï¸âƒ£ LLM ë° í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "prompt = PromptTemplate(\n",
      "    input_variables=[\"question\", \"context\"],\n",
      "    template=\"\"\"\n",
      "    ì§ˆë¬¸: {question}\n",
      "    ê´€ë ¨ ë¬¸ì„œ:\n",
      "    {context}\n",
      "    ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
      "    \"\"\"\n",
      ")\n",
      "\n",
      "# 4ï¸âƒ£ RetrievalQA ì²´ì¸ êµ¬ì„±\n",
      "qa_chain = RetrievalQAChain.from_chain_type(\n",
      "    llm=llm,\n",
      "    chain_type=\"stuff\",   # \"map_reduce\", \"refine\" ë“± ì„ íƒ ê°€ëŠ¥\n",
      "    retriever=retriever,\n",
      "    combine_prompt=prompt,\n",
      ")\n",
      "\n",
      "# 5ï¸âƒ£ ì§ˆë¬¸í•˜ê¸°\n",
      "result = qa_chain.run(\"ìš°ë¦¬ ì„œë¹„ìŠ¤ì˜ í™˜ë¶ˆ ì •ì±…ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\")\n",
      "print(result)\n",
      "```\n",
      "\n",
      "> **í•µì‹¬ í¬ì¸íŠ¸**  \n",
      "> - `retriever`ê°€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ(3ê°œ)ë§Œ ë°˜í™˜í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ `prompt`ì— ì‚½ì…í•´ LLMì´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.  \n",
      "> - `chain_type=\"stuff\"` ì€ ê°„ë‹¨íˆ ëª¨ë“  ë¬¸ì„œë¥¼ í•œ ë²ˆì— LLMì— ì „ë‹¬í•˜ëŠ” ë°©ì‹ì´ë©°, ëŒ€ê·œëª¨ ë¬¸ì„œë¼ë©´ `map_reduce` ë“±ìœ¼ë¡œ ë¶„ì‚° ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## ì—ì´ì „íŠ¸ ì˜ˆì‹œ (ê²€ìƒ‰ + ê³„ì‚°)\n",
      "\n",
      "```python\n",
      "from langchain.agents import initialize_agent, Tool\n",
      "from langchain.tools import SerpAPIWrapper, PythonREPLTool\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "# 1ï¸âƒ£ íˆ´ ì •ì˜\n",
      "search_tool = Tool(\n",
      "    name=\"Search\",\n",
      "    func=SerpAPIWrapper().run,\n",
      "    description=\"ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜í™˜í•œë‹¤.\"\n",
      ")\n",
      "\n",
      "calc_tool = Tool(\n",
      "    name=\"Calculator\",\n",
      "    func=PythonREPLTool().run,\n",
      "    description=\"ìˆ˜í•™ì‹ì´ë‚˜ ê°„ë‹¨í•œ íŒŒì´ì¬ ì½”ë“œë¥¼ ì‹¤í–‰í•œë‹¤.\"\n",
      ")\n",
      "\n",
      "# 2ï¸âƒ£ LLM ë° ì—ì´ì „íŠ¸ ì´ˆê¸°í™”\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "agent = initialize_agent(\n",
      "    tools=[search_tool, calc_tool],\n",
      "    llm=llm,\n",
      "    agent_type=\"zero-shot-react-description\",  # ReAct ë°©ì‹\n",
      "    verbose=True\n",
      ")\n",
      "\n",
      "# 3ï¸âƒ£ ì‚¬ìš©\n",
      "answer = agent.run(\"2024ë…„ í•œêµ­ì˜ ì¸êµ¬ëŠ” ì–¼ë§ˆì´ê³ , ê·¸ ìˆ«ìë¥¼ 2ë¡œ ë‚˜ëˆ„ë©´ ì–¼ë§ˆì¸ê°€?\")\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "- ì—ì´ì „íŠ¸ëŠ” **â€œSearchâ€**ì™€ **â€œCalculatorâ€** ì¤‘ ì–´ë–¤ íˆ´ì„ ì‚¬ìš©í• ì§€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•˜ê³ , ê²°ê³¼ë¥¼ ì¡°í•©í•´ ìµœì¢… ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## LangChain ìƒíƒœê³„\n",
      "\n",
      "| ì˜ì—­ | ì£¼ìš” íŒ¨í‚¤ì§€/í”„ë¡œì íŠ¸ |\n",
      "|------|---------------------|\n",
      "| **ì½”ì–´** | `langchain`, `langchain-community` |\n",
      "| **ë²¡í„° DB í†µí•©** | FAISS, Pinecone, Weaviate, Milvus, Chroma ë“± |\n",
      "| **LLM ì¸í„°í˜ì´ìŠ¤** | OpenAI, Anthropic, Cohere, Llamaâ€‘CPP, Vertex AI ë“± |\n",
      "| **íˆ´Â·ì—ì´ì „íŠ¸** | SerpAPI, Wikipedia, Google Search, SQLDatabase, PythonREPL, Zapier ë“± |\n",
      "| **ëª¨ë‹ˆí„°ë§Â·ë””ë²„ê¹…** | **LangSmith** (íŠ¸ë ˆì´ì‹±, í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬, ë¹„ìš© ë¶„ì„) |\n",
      "| **í”„ë¡ íŠ¸ì—”ë“œ** | Streamlit, Gradio, Next.js(React)ìš© í…œí”Œë¦¿ |\n",
      "| **ë°°í¬** | Docker, AWS Lambda, GCP Cloud Run, LangChain Hub (ê³µìœ  ì²´ì¸) |\n",
      "\n",
      "---\n",
      "\n",
      "## ì–¸ì œ LangChainì„ ì‚¬ìš©í•˜ë©´ ì¢‹ì€ê°€?\n",
      "\n",
      "| ìƒí™© | ì´ìœ  |\n",
      "|------|------|\n",
      "| **ë³µí•©ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§** (ê²€ìƒ‰ â†’ ìš”ì•½ â†’ ì˜ì‚¬ê²°ì •) | ì—¬ëŸ¬ ë‹¨ê³„ì™€ ì™¸ë¶€ ì‹œìŠ¤í…œì„ í•œ ë²ˆì— ì—°ê²°í•´ ê´€ë¦¬í•˜ê¸° ì‰¬ì›€ |\n",
      "| **ëŒ€ê·œëª¨ ë¬¸ì„œ ê¸°ë°˜ Q&A** | ë²¡í„° ê²€ìƒ‰ + ë©”ëª¨ë¦¬ + í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì†ì‰½ê²Œ êµ¬ì„± |\n",
      "| **ë™ì  íˆ´ ì„ íƒì´ í•„ìš”í•œ ì—ì´ì „íŠ¸** | â€œë‚ ì”¨ ì•Œë ¤ì¤˜â€, â€œì£¼ì‹ ê°€ê²© ì¡°íšŒâ€, â€œì½”ë“œ ì‹¤í–‰â€ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì„ í•˜ë‚˜ì˜ ì¸í„°í˜ì´ìŠ¤ì— í†µí•© |\n",
      "| **í”„ë¡¬í”„íŠ¸ ì‹¤í—˜Â·ë²„ì „ ê´€ë¦¬** | `PromptTemplate` + LangSmithìœ¼ë¡œ ì‹¤í—˜ ê²°ê³¼ì™€ ë¹„ìš©ì„ ì¶”ì  |\n",
      "| **íŒ€Â·í”„ë¡œì íŠ¸ ê·œëª¨ê°€ ì»¤ì§ˆ ë•Œ** | ëª¨ë“ˆí™”ëœ ì²´ì¸, ì¬ì‚¬ìš© ê°€ëŠ¥í•œ íˆ´, í‘œì¤€í™”ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ í˜‘ì—…ì´ ìš©ì´ |\n",
      "\n",
      "---\n",
      "\n",
      "## ì‹œì‘í•˜ê¸° (ê°„ë‹¨ ê°€ì´ë“œ)\n",
      "\n",
      "1. **Python í™˜ê²½ ì¤€ë¹„**  \n",
      "   ```bash\n",
      "   python -m venv venv\n",
      "   source venv/bin/activate\n",
      "   pip install \"langchain[all]\"   # ëª¨ë“  optional deps ì„¤ì¹˜\n",
      "   ```\n",
      "\n",
      "2. **API í‚¤ ì„¤ì •** (ì˜ˆ: OpenAI)  \n",
      "   ```bash\n",
      "   export OPENAI_API_KEY=\"sk-...\"\n",
      "   ```\n",
      "\n",
      "3. **ì²« ì²´ì¸ ì‹¤í–‰** â€“ ìœ„ â€œë¬¸ì„œ ê¸°ë°˜ Q&Aâ€ ì˜ˆì‹œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ ì‹¤í–‰í•˜ë©´ ë°”ë¡œ ë™ì‘í•©ë‹ˆë‹¤.\n",
      "\n",
      "4. **ì½”ë“œ ê³µìœ Â·ì¬ì‚¬ìš©** â€“ `langchain hub`ì— ì²´ì¸ í…œí”Œë¦¿ì„ ì˜¬ë¦¬ë©´ íŒ€ì›Â·ì»¤ë®¤ë‹ˆí‹°ì™€ ì‰½ê²Œ ê³µìœ  ê°€ëŠ¥.\n",
      "\n",
      "---\n",
      "\n",
      "## ë§ˆë¬´ë¦¬\n",
      "\n",
      "- **LangChain**ì€ LLMì„ **â€˜ìŠ¤ë§ˆíŠ¸ ì—”ì§„â€™**ì´ ì•„ë‹ˆë¼ **â€˜í”ŒëŸ¬ê·¸ì¸ ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸â€™** ë¡œ ë‹¤ë£¨ì–´, ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§, ë°ì´í„° íŒŒì´í”„ë¼ì¸, ìë™í™” ì—ì´ì „íŠ¸ ë“±ìœ¼ë¡œ í™•ì¥í•  ìˆ˜ ìˆê²Œ í•´ ì¤ë‹ˆë‹¤.  \n",
      "- í•µì‹¬ì€ **Chain, Agent, Memory, Tool**ì´ë¼ëŠ” ë„¤ ê°€ì§€ ì¶”ìƒí™”ì´ë©°, ì´ë¥¼ ì¡°í•©í•˜ë©´ ê±°ì˜ ëª¨ë“  â€œLLM ê¸°ë°˜ ì„œë¹„ìŠ¤â€ë¥¼ ë¹ ë¥´ê²Œ í”„ë¡œí† íƒ€ì´í•‘í•˜ê³ , í”„ë¡œë•ì…˜ ìˆ˜ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸš€\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
