{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello LangChain!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsk_n\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "#load_dotenv(dotenv_path='.env')\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt + llm + output \n",
    "\n",
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , \n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "System: 당신은 개발자입니다.\n",
      "Human: 파이썬은 무엇인가요? 자세하게 설명해주세요\n"
     ]
    }
   ],
   "source": [
    "prompt_text = prompt.format(input=\"파이썬은 무엇인가요? 자세하게 설명해주세요\")\n",
    "print(type(prompt_text))\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_openai.chat_models.base.ChatOpenAI'>\n",
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000002669E0B9E50> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002669E0BB6E0> root_client=<openai.OpenAI object at 0x000002669E0BB500> root_async_client=<openai.AsyncOpenAI object at 0x000002669E0BB560> model_name='openai/gpt-oss-120b' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "#llm = ChatOpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    #model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(type(llm))\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(\"응답:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "first=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002669E0B9E50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002669E0BB6E0>, root_client=<openai.OpenAI object at 0x000002669E0BB500>, root_async_client=<openai.AsyncOpenAI object at 0x000002669E0BB560>, model_name='openai/gpt-oss-120b', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "print(type(chain))\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\":\"LangChain은 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "## LangChain이란?\n",
      "\n",
      "**LangChain**은 대형 언어 모델(LLM, Large Language Model)을 **애플리케이션 수준**으로 활용할 수 있게 해 주는 **오픈소스 프레임워크**입니다.  \n",
      "LLM을 단순히 “텍스트를 생성하는 엔진”으로 쓰는 것이 아니라, **데이터베이스, API, 파일 시스템, 사용자 인터페이스 등 다양한 외부 자원**과 결합해 **복합적인 워크플로우**를 만들 수 있도록 도와줍니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 핵심 개념\n",
      "\n",
      "| 개념 | 설명 | 예시 |\n",
      "|------|------|------|\n",
      "| **Chain** | 여러 LLM 호출·프롬프트·처리 단계를 **순차·조건부·반복**으로 연결한 흐름 | 질문 → 검색 → 요약 → 답변 |\n",
      "| **PromptTemplate** | 변수(`{question}` 등)를 삽입해 동적으로 프롬프트를 생성 | `\"다음 질문에 답해 주세요: {question}\"` |\n",
      "| **LLM** | 실제 텍스트를 생성하는 모델 (OpenAI GPT‑4, Anthropic Claude, Llama 등) | `OpenAIChat`, `ChatAnthropic` |\n",
      "| **Memory** | 대화·작업 이력을 **상태**로 저장해 다음 호출에 활용 | 대화 기록, 작업 진행 상황 |\n",
      "| **Agents** | **툴(tool)**(검색 엔진, 데이터베이스, 코드 실행 등)을 **동적으로 선택**해 작업 수행 | “날씨를 알려줘” → 검색 툴 호출 → 결과 반환 |\n",
      "| **Tools** | 외부 서비스·함수·API를 래핑한 **플러그인** | `SerpAPI`, `SQLDatabase`, `PythonREPL` |\n",
      "| **Retriever** | 텍스트/문서 컬렉션에서 **관련 문서**를 검색해 LLM에 제공 | 벡터 DB(FAISS, Pinecone) 기반 검색 |\n",
      "| **Callbacks** | 실행 흐름을 **감시·로깅·시각화**할 수 있는 훅 | LangSmith(모니터링), Streamlit UI 연동 |\n",
      "\n",
      "---\n",
      "\n",
      "## 주요 사용 시나리오\n",
      "\n",
      "| 시나리오 | 구현 방법 (LangChain) |\n",
      "|----------|----------------------|\n",
      "| **질의응답(Q&A) 챗봇** | `RetrievalQAChain` → 벡터 DB에서 문서 검색 → LLM이 답변 |\n",
      "| **에이전트 기반 자동화** | `AgentExecutor` + 여러 `Tool` (검색, 코드 실행, DB 쿼리) |\n",
      "| **문서 요약·분석** | `MapReduceDocumentsChain` 또는 `RefineDocumentsChain` |\n",
      "| **프롬프트 엔지니어링** | `PromptTemplate` + `LLMChain` 로 반복 테스트 |\n",
      "| **멀티스텝 워크플로우** | `SequentialChain` 혹은 `GraphChain` 으로 복합 로직 구현 |\n",
      "| **대화 메모리** | `ConversationBufferMemory`, `ConversationSummaryMemory` 등 사용 |\n",
      "\n",
      "---\n",
      "\n",
      "## 기본 코드 예시 (Python)\n",
      "\n",
      "```python\n",
      "from langchain import OpenAI, PromptTemplate, LLMChain\n",
      "from langchain.chains import RetrievalQAChain\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.document_loaders import TextLoader\n",
      "\n",
      "# 1️⃣ 문서 로드 & 임베딩 → 벡터스토어 구축\n",
      "loader = TextLoader(\"data/faq.txt\")\n",
      "docs = loader.load()\n",
      "embeddings = OpenAIEmbeddings()\n",
      "vectorstore = FAISS.from_documents(docs, embeddings)\n",
      "\n",
      "# 2️⃣ Retriever 생성\n",
      "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
      "\n",
      "# 3️⃣ LLM 및 프롬프트 템플릿 정의\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "prompt = PromptTemplate(\n",
      "    input_variables=[\"question\", \"context\"],\n",
      "    template=\"\"\"\n",
      "    질문: {question}\n",
      "    관련 문서:\n",
      "    {context}\n",
      "    위 정보를 바탕으로 정확하고 간결하게 답변해 주세요.\n",
      "    \"\"\"\n",
      ")\n",
      "\n",
      "# 4️⃣ RetrievalQA 체인 구성\n",
      "qa_chain = RetrievalQAChain.from_chain_type(\n",
      "    llm=llm,\n",
      "    chain_type=\"stuff\",   # \"map_reduce\", \"refine\" 등 선택 가능\n",
      "    retriever=retriever,\n",
      "    combine_prompt=prompt,\n",
      ")\n",
      "\n",
      "# 5️⃣ 질문하기\n",
      "result = qa_chain.run(\"우리 서비스의 환불 정책은 어떻게 되나요?\")\n",
      "print(result)\n",
      "```\n",
      "\n",
      "> **핵심 포인트**  \n",
      "> - `retriever`가 가장 관련성 높은 문서(3개)만 반환하고, 그 결과를 `prompt`에 삽입해 LLM이 답변을 생성합니다.  \n",
      "> - `chain_type=\"stuff\"` 은 간단히 모든 문서를 한 번에 LLM에 전달하는 방식이며, 대규모 문서라면 `map_reduce` 등으로 분산 처리할 수 있습니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 에이전트 예시 (검색 + 계산)\n",
      "\n",
      "```python\n",
      "from langchain.agents import initialize_agent, Tool\n",
      "from langchain.tools import SerpAPIWrapper, PythonREPLTool\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "# 1️⃣ 툴 정의\n",
      "search_tool = Tool(\n",
      "    name=\"Search\",\n",
      "    func=SerpAPIWrapper().run,\n",
      "    description=\"주어진 질문에 대해 웹 검색 결과를 반환한다.\"\n",
      ")\n",
      "\n",
      "calc_tool = Tool(\n",
      "    name=\"Calculator\",\n",
      "    func=PythonREPLTool().run,\n",
      "    description=\"수학식이나 간단한 파이썬 코드를 실행한다.\"\n",
      ")\n",
      "\n",
      "# 2️⃣ LLM 및 에이전트 초기화\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "agent = initialize_agent(\n",
      "    tools=[search_tool, calc_tool],\n",
      "    llm=llm,\n",
      "    agent_type=\"zero-shot-react-description\",  # ReAct 방식\n",
      "    verbose=True\n",
      ")\n",
      "\n",
      "# 3️⃣ 사용\n",
      "answer = agent.run(\"2024년 한국의 인구는 얼마이고, 그 숫자를 2로 나누면 얼마인가?\")\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "- 에이전트는 **“Search”**와 **“Calculator”** 중 어떤 툴을 사용할지 스스로 판단하고, 결과를 조합해 최종 답변을 제공합니다.\n",
      "\n",
      "---\n",
      "\n",
      "## LangChain 생태계\n",
      "\n",
      "| 영역 | 주요 패키지/프로젝트 |\n",
      "|------|---------------------|\n",
      "| **코어** | `langchain`, `langchain-community` |\n",
      "| **벡터 DB 통합** | FAISS, Pinecone, Weaviate, Milvus, Chroma 등 |\n",
      "| **LLM 인터페이스** | OpenAI, Anthropic, Cohere, Llama‑CPP, Vertex AI 등 |\n",
      "| **툴·에이전트** | SerpAPI, Wikipedia, Google Search, SQLDatabase, PythonREPL, Zapier 등 |\n",
      "| **모니터링·디버깅** | **LangSmith** (트레이싱, 프롬프트 버전 관리, 비용 분석) |\n",
      "| **프론트엔드** | Streamlit, Gradio, Next.js(React)용 템플릿 |\n",
      "| **배포** | Docker, AWS Lambda, GCP Cloud Run, LangChain Hub (공유 체인) |\n",
      "\n",
      "---\n",
      "\n",
      "## 언제 LangChain을 사용하면 좋은가?\n",
      "\n",
      "| 상황 | 이유 |\n",
      "|------|------|\n",
      "| **복합적인 비즈니스 로직** (검색 → 요약 → 의사결정) | 여러 단계와 외부 시스템을 한 번에 연결해 관리하기 쉬움 |\n",
      "| **대규모 문서 기반 Q&A** | 벡터 검색 + 메모리 + 프롬프트 템플릿을 손쉽게 구성 |\n",
      "| **동적 툴 선택이 필요한 에이전트** | “날씨 알려줘”, “주식 가격 조회”, “코드 실행” 등 다양한 작업을 하나의 인터페이스에 통합 |\n",
      "| **프롬프트 실험·버전 관리** | `PromptTemplate` + LangSmith으로 실험 결과와 비용을 추적 |\n",
      "| **팀·프로젝트 규모가 커질 때** | 모듈화된 체인, 재사용 가능한 툴, 표준화된 인터페이스로 협업이 용이 |\n",
      "\n",
      "---\n",
      "\n",
      "## 시작하기 (간단 가이드)\n",
      "\n",
      "1. **Python 환경 준비**  \n",
      "   ```bash\n",
      "   python -m venv venv\n",
      "   source venv/bin/activate\n",
      "   pip install \"langchain[all]\"   # 모든 optional deps 설치\n",
      "   ```\n",
      "\n",
      "2. **API 키 설정** (예: OpenAI)  \n",
      "   ```bash\n",
      "   export OPENAI_API_KEY=\"sk-...\"\n",
      "   ```\n",
      "\n",
      "3. **첫 체인 실행** – 위 “문서 기반 Q&A” 예시를 그대로 복사해 실행하면 바로 동작합니다.\n",
      "\n",
      "4. **코드 공유·재사용** – `langchain hub`에 체인 템플릿을 올리면 팀원·커뮤니티와 쉽게 공유 가능.\n",
      "\n",
      "---\n",
      "\n",
      "## 마무리\n",
      "\n",
      "- **LangChain**은 LLM을 **‘스마트 엔진’**이 아니라 **‘플러그인 가능한 컴포넌트’** 로 다루어, 실제 비즈니스 로직, 데이터 파이프라인, 자동화 에이전트 등으로 확장할 수 있게 해 줍니다.  \n",
      "- 핵심은 **Chain, Agent, Memory, Tool**이라는 네 가지 추상화이며, 이를 조합하면 거의 모든 “LLM 기반 서비스”를 빠르게 프로토타이핑하고, 프로덕션 수준으로 스케일링할 수 있습니다.\n",
      "\n",
      "궁금한 점이 있으면 언제든 물어보세요! 🚀\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
