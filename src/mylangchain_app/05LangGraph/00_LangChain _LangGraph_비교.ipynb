{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LangChain vs LangGraph (feat. LangGraph 개념 설명)\n",
        "* LangGraph의 개념과 주요 기능을 이해하고, 차이점을 비교합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# poetry add langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sk\n",
            "WA\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "print(OPENAI_API_KEY[:2])\n",
        "\n",
        "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
        "print(UPSTAGE_API_KEY[30:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "client=<openai.resources.chat.completions.completions.Completions object at 0x000001F20EAB35F0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F20EAF4830> model_name='solar-pro' temperature=0.5 model_kwargs={} upstage_api_key=SecretStr('**********') upstage_api_base='https://api.upstage.ai/v1'\n",
            "**LangGraph**는 **대규모 언어 모델(LLM)**을 기반으로 한 **복잡한 대화형 애플리케이션**을 구축하기 위한 오픈소스 프레임워크입니다. 특히 **상태(State) 관리**와 **멀티턴 대화 흐름**을 체계적으로 제어할 수 있도록 설계되었으며, LangChain과 통합되어 확장성을 제공합니다.\n",
            "\n",
            "### 핵심 특징:\n",
            "1. **상태 기반 대화 관리**  \n",
            "   - 사용자 세션의 상태(메모리, 컨텍스트)를 자동으로 추적하여 복잡한 대화 흐름을 구현합니다.  \n",
            "   - 예: 이전 대화 기록을 기반으로 다음 응답을 생성하거나 의사 결정 트리를 구축.\n",
            "\n",
            "2. **그래프 기반 아키텍처**  \n",
            "   - 대화를 **노드(Node)**와 **엣지(Edge)**로 구성된 그래프 구조로 모델링합니다.  \n",
            "   - 분기 조건, 루프, 병렬 처리 등 유연한 흐름 설계가 가능합니다.\n",
            "\n",
            "3. **LangChain과의 통합**  \n",
            "   - LangChain의 도구 호출, 메모리, 체인 기능을 활용해 LLM 기반 애플리케이션을 쉽게 확장할 수 있습니다.\n",
            "\n",
            "4. **확장성**  \n",
            "   - 커스텀 노드/엣지를 추가해 비즈니스 로직이나 외부 API 연동이 가능합니다.\n",
            "\n",
            "### 사용 사례:\n",
            "- **챗봇**: 다단계 폼 작성, 고객 지원 자동화  \n",
            "- **에이전트 시스템**: 문서 분석 → 요약 → 사용자 질의 응답  \n",
            "- **게임/튜토리얼**: 상태 변화에 따른 동적 대화 흐름  \n",
            "\n",
            "### 예시 코드 (간단한 그래프):\n",
            "```python\n",
            "from langgraph import State, Graph\n",
            "\n",
            "state = State()\n",
            "graph = Graph(state)\n",
            "\n",
            "@graph.node\n",
            "def start(state):\n",
            "    state[\"step\"] = 1\n",
            "    return {\"next\": {\"where_to_go\": \"question_1\"}}\n",
            "\n",
            "@graph.node\n",
            "def question_1(state):\n",
            "    if state[\"step\"] == 1:\n",
            "        return {\"next\": {\"where_to_go\": \"end\"}}\n",
            "\n",
            "graph.add_node(\"start\", start)\n",
            "graph.add_node(\"question_1\", question_1)\n",
            "graph.add_edge(\"start\", \"question_1\")\n",
            "graph.compile()\n",
            "```\n",
            "\n",
            "LangGraph는 LLM 애플리케이션의 **복잡성**을 추상화하고 **유지보수성**을 높이는 데 중점을 둔 프레임워크입니다. 공식 문서(https://langchain-ai.github.io/langgraph/)에서 더 자세한 정보를 확인할 수 있습니다.\n"
          ]
        }
      ],
      "source": [
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "# llm = ChatOpenAI(model='gpt-4o-mini') # 테스트의 경우에는 작은 모델을 사용합니다\n",
        "\n",
        "from langchain_upstage import ChatUpstage\n",
        "llm = ChatUpstage(\n",
        "        model=\"solar-pro\",\n",
        "        base_url=\"https://api.upstage.ai/v1\",\n",
        "        temperature=0.5\n",
        "    )\n",
        "print(llm)\n",
        "\n",
        "query = 'LangGraph는 무엇인가요?'\n",
        "ai_msg = llm.invoke(query)\n",
        "print(ai_msg.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LangGraph의 기본개념\n",
        "* `state`는 LangGraph 에이전트의 state를 나타내는 데이터 구조입니다.\n",
        "* `state`는 `TypedDict`를 사용하여 정의되며, 이는 Python의 타입 힌팅을 통해 구조를 명확히 합니다.\n",
        "    * 간단하게 `messages`라는 필드만 있습니다.\n",
        "    * 필요에 따라 다양한 값들을 활용할 수 있습니다.\n",
        "* `state`는 에이전트의 동작을 결정하는 데 사용되며, 각 노드에서 state를 업데이트하거나 참조할 수 있습니다.\n",
        "* `state`는 LangGraph의 노드 간에 전달되며, 에이전트의 state 전이를 관리합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Annotated # 타입 힌트를 사용하기 위해 \n",
        "from typing_extensions import TypedDict # 구조화된 딕셔너리 타입을 정의하기 위해 \n",
        "\n",
        "from langgraph.graph.message import add_messages \n",
        "from langchain_core.messages import AnyMessage # LangChain에서 사용하는 모든 종류의 메시지(예: HumanMessage, AIMessage, ToolMessage)\n",
        "\n",
        "# AgentState는 에이전트의 현재 상태를 나타내는 딕셔너리 타입을 정의합니다.\n",
        "# TypedDict를 사용하면 딕셔너리가 어떤 키와 값 타입을 가져야 하는지 명확하게 지정할 수 있습니다.\n",
        "class AgentState(TypedDict):\n",
        "    # 'messages' 키는 에이전트의 대화 기록을 저장합니다.\n",
        "    # 이 목록에는 LangChain 메시지 객체(AnyMessage)가 들어갑니다.\n",
        "    # LangGraph가 이 상태를 처리할 때, 새로운 메시지가 추가되면\n",
        "    # 기존 메시지 목록의 끝에 자동으로 추가되도록(append) 설정합니다.\n",
        "    messages: list[Annotated[AnyMessage, add_messages]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 위에 선언한 `AgentState`를 활용하여 `StateGraph`를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'langgraph.graph.state.StateGraph'>\n"
          ]
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph\n",
        "\n",
        "graph_builder = StateGraph(AgentState)\n",
        "print(type(graph_builder))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `graph`에 추가할 `node`를 생성합니다\n",
        "-  `node`는 LangGraph에서 실행되는 개별적인 작업 단위를 의미합니다. \n",
        "    - 각 노드는 특정 기능을 수행하는 독립적인 컴포넌트로, 예를 들어 텍스트 생성, 데이터 처리, 또는 의사 결정과 같은 작업을 담당할 수 있습니다.\n",
        "    - `node`는 기본적으로 함수(function)로 정의되고, 뒤에서 다루지만 다른 에이전트(agent)를 활용할 수도 있습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Node 역할을 하는 함수의 인자로 state 객체를 사용함  LLM을 호출하는 노드\n",
        "def generate(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    `generate` 노드는 사용자의 질문을 받아서 응답을 생성하는 노드입니다.\n",
        "    \"\"\"\n",
        "    messages = state['messages']\n",
        "    ai_message = llm.invoke(messages)\n",
        "    return {'messages': [ai_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `node`를 생성한 후에 `edge`로 연결합니다\n",
        "- `edge`는 노드들 사이의 연결을 나타내며, 데이터와 제어 흐름의 경로를 정의합니다. \n",
        "    - 엣지를 통해 한 노드의 출력이 다음 노드의 입력으로 전달되어, 전체적인 워크플로우가 형성됩니다.\n",
        "    - `node`와 `edge`의 조합은 방향성 그래프(Directed Graph)를 형성하며, 이를 통해 복잡한 AI 에이전트의 행동 흐름을 구조화할 수 있습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x1f20ee9bd70>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# LLM을 호출하는 generate 함수를 Node로 추가함\n",
        "graph_builder.add_node('generate', generate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 모든 그래프는 `START(시작)`와 `END(종료)`가 있습니다\n",
        "    - `END`를 explicit하게 선언하지 않는 경우도 종종 있지만, 가독성을 위해 작성해주는 것을 권장합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x1f20ee9bd70>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langgraph.graph import START, END\n",
        "\n",
        "graph_builder.add_edge(START, 'generate')\n",
        "graph_builder.add_edge('generate', END)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `node`를 생성하고 `edge`로 연결한 후에 `compile` 메서드를 호출하여 `Graph`를 생성합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'langgraph.graph.state.CompiledStateGraph'>\n"
          ]
        }
      ],
      "source": [
        "graph = graph_builder.compile()\n",
        "print(type(graph))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `compile` 후에는 그래프를 시각화하여 확인할 수 있습니다\n",
        "- 의도한대로 그래프가 생성됐는지 확인하는 습관을 기르는 것이 좋습니다\n",
        "    - `git`에서 코드 작업물을 commit하기 전에 `git diff`를 통해 변경사항을 확인하는 것과 같습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#from IPython.display import display, Image\n",
        "\n",
        "#display(Image(graph.get_graph().draw_mermaid_png(max_retries=5, retry_delay=2.0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mermaid Code:\n",
            "---\n",
            "config:\n",
            "  flowchart:\n",
            "    curve: linear\n",
            "---\n",
            "graph TD;\n",
            "\t__start__([<p>__start__</p>]):::first\n",
            "\tgenerate(generate)\n",
            "\t__end__([<p>__end__</p>]):::last\n",
            "\t__start__ --> generate;\n",
            "\tgenerate --> __end__;\n",
            "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
            "\tclassDef first fill-opacity:0\n",
            "\tclassDef last fill:#bfb6fc\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 대체 방법\n",
        "mermaid_code = graph.get_graph().draw_mermaid()\n",
        "print(\"Mermaid Code:\")\n",
        "print(mermaid_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* https://mermaid.live/ 에서  mermain_code 로 직접 확인한다.\n",
        "\n",
        "* [Graph 이미지](https://mermaidchart.com/play?utm_source=mermaid_live_editor&utm_medium=share#pako:eNpVkN1ugjAUx1-lObvRBLCAVq3Gm_kIu9q6mAqn0AwKKSWZM777KirR3vR8_f7n4wxZkyNwKKxsS_Kx3wgj3OHQOWn9N_natrvR287a3feUc6607dy1sECDVjqcPIzpDUeTj_Bgj2glb-QoSsJwRx745ll0yNz5IZF5uNujIjkq2VeOKF1V_E0liioVVNpgWKIuSsfjKHkBhoGH8rBpZabdidOXgutYd7mjOjKVQeBPonPgzvYYQI22llcXzsIQIsCVWKMA7s1c2h8Bwlw800rz2TT1A7NNX5TAlaw67_Vt7tfaa-mPXY9R6xdE-970xgFP4kED-Bl-gaeURYyyNEmXbJGuaDIP4OTDLIqT-XoVp2tK54ytLwH8DV1ptFou6NOLL_-94J8W) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "<class 'langchain_core.messages.ai.AIMessage'>\n",
            "[AIMessage(content='**LangGraph**와 **LangChain**은 모두 LLM(대형 언어 모델) 기반 애플리케이션 개발을 위한 프레임워크이지만, 목적과 기능에서 차이가 있습니다.  \\n\\n### 1. **LangGraph란?**  \\n- **상태(State) 기반 LLM 애플리케이션**을 구축하기 위한 프레임워크로, **대화형 에이전트**나 **복잡한 워크플로우**를 설계할 때 유용합니다.  \\n- **그래프 구조**를 사용해 상태 전이를 명시적으로 정의할 수 있어, 다중 턴 대화, 조건부 분기, 메모리 관리 등이 필요한 시나리오에 적합합니다.  \\n- LangChain의 확장 도구로 개발되었으며, `LangChain Core`와 통합되어 동작합니다.  \\n\\n**주요 특징:**  \\n- **상태 관리**: 대화 기록, 사용자 입력, 시스템 상태 등을 추적합니다.  \\n- **그래프 기반 워크플로우**: 노드(에이전트/툴)와 엣지(전이 조건)로 흐름을 시각화합니다.  \\n- **비동기 지원**: 실시간 애플리케이션에 최적화되었습니다.  \\n\\n---\\n\\n### 2. **LangChain과의 차이점**  \\n| **기준**          | **LangChain**                          | **LangGraph**                          |\\n|-------------------|----------------------------------------|----------------------------------------|\\n| **목적**          | LLM 기반 애플리케이션의 **모듈 조합** (체인, 에이전트, 메모리 등) | **상태 전이**와 **대화 흐름** 관리       |\\n| **구조**          | 선형/분기형 체인                        | 그래프 기반 상태 머신                   |\\n| **사용 사례**     | 단일 턴 Q&A, 문서 요약, 간단한 에이전트 | 다중 턴 대화, 게임, 복잡한 워크플로우    |\\n| **상태 관리**     | 기본 메모리 제공 (ConversationBuffer 등) | 명시적인 상태 객체(`State`)와 전이 로직 |\\n| **통합**          | 다양한 LLM/툴과의 호환성 강조           | LangChain Core와 결합해 확장성 제공     |\\n\\n---\\n\\n### 3. **예시: LangGraph vs. LangChain**  \\n- **LangChain**으로 \"문서 검색 → 답변 생성\" 체인을 구축한다면,  \\n- **LangGraph**로는 \"사용자 질문 → 검색 필요 여부 판단 → 검색 실행/생략 → 답변 생성\"과 같은 **조건부 흐름**을 설계할 수 있습니다.  \\n\\n```python\\n# LangGraph 예시 (의사 코드)\\nfrom langgraph import State, Graph\\n\\nstate = State(question=\"What\\'s the weather?\", step=0)\\ngraph = {\\n    \"Check_Weather\": {\"next\": {\"search_needed\": \"Search_Web\", \"default\": \"Answer\"}},\\n    \"Search_Web\": {\"next\": \"Answer\"},\\n    \"Answer\": {\"next\": \"end\"}\\n}\\n```\\n\\n---\\n\\n### 4. **결론**  \\n- **LangChain**은 LLM 애플리케이션의 **구성 요소**를 제공하는 반면,  \\n- **LangGraph**는 **대화 흐름**과 **상태 관리**를 위한 고급 도구를 제공합니다.  \\n- 복잡한 멀티턴 애플리케이션에서는 두 프레임워크를 함께 사용하는 경우가 많습니다.  \\n\\n> 🔍 **참고**: LangGraph는 LangChain 생태계의 일부로, [공식 문서](https://langchain.com/docs/langgraph)에서 자세한 예제를 확인할 수 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 631, 'prompt_tokens': 25, 'total_tokens': 656, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'solar-pro2-250909', 'system_fingerprint': None, 'id': 'e69c8764-4d50-4746-92c2-4e79bbcf46cb', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--8ced7667-4993-476f-ab2b-1be2e933c1e8-0', usage_metadata={'input_tokens': 25, 'output_tokens': 631, 'total_tokens': 656, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "query = 'LangGraph는 무엇인가요? LangChain과의 차이점은 무엇인가요?'\n",
        "initial_state = {'messages': [HumanMessage(query)]}\n",
        "result = graph.invoke(initial_state)\n",
        "\n",
        "print(type(result))\n",
        "print(type(result['messages'][0]))\n",
        "print(result['messages'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2개의 AI 에이전트 협력하기\n",
        "* 첫번째 AI 에이전트\n",
        "    * 사용자의 질문을 분석하고 핵심 키워드와 배경 정보를 추가하는 역할\n",
        "* 두번째 AI 에이전트\n",
        "    * 첫번째 에이전트가 제공한 정보를 기반으로 좀 더 자세한 답변을 생성하는 역할    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "solar-pro\n"
          ]
        }
      ],
      "source": [
        "print(llm.model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph\n",
        "from pprint import pprint\n",
        "\n",
        "#첫번째 AI 에이전트\n",
        "def agent_1(state):\n",
        "    \"\"\"사용자의 질문을 분석하고 핵심 키워드아 배경 정보를 추가하는 역할\"\"\"\n",
        "    query = state['query']\n",
        "    \n",
        "    keywords = llm.invoke(f'질문: {query}\\n 이 질문에서 핵심 키워드를 3~5개 추출해 주세요.')\n",
        "\n",
        "     # 질문과 관련된 배경 정보 제공\n",
        "    background_info = llm.invoke(f\"질문: {query}\\n 이 질문을 이해하는 데 도움이 될 만한 추가 정보를 제공해 주세요.\")\n",
        "\n",
        "    print(f\"\\n[Agent 1] 원본 질문: {query}\")\n",
        "    print(f\"[Agent 1] 핵심 키워드: {keywords}\")\n",
        "    print(f\"[Agent 1] 배경 정보: {background_info}\\n\")\n",
        "\n",
        "    return {\"refined_query\": query, \"keywords\": keywords, \"background_info\": background_info}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 두번째 AI 에이전트\n",
        "def agent_2(state):\n",
        "    \"\"\"첫번째 에이전트가 제공한 정보를 기반으로 좀 더 자세한 답변을 생성하는 역할\"\"\"\n",
        "    refined_query = state['refined_query']\n",
        "    keywords = state['keywords']\n",
        "    background_info = state['background_info']\n",
        "\n",
        "    # Agent 1이 제공한 정보를 활용하여 최종 답변 생성\n",
        "    final_response = llm.invoke(\n",
        "        f\"질문: {refined_query}\\n\"\n",
        "        f\"핵심 키워드: {keywords}\\n\"\n",
        "        f\"배경 정보: {background_info}\\n\"\n",
        "        f\"위 정보를 바탕으로 질문에 대해 깊이 있는 답변을 작성해 주세요.\"\n",
        "    )\n",
        "\n",
        "    print(f\"[Agent 2] 최종 답변 생성 완료\\n\")\n",
        "    \n",
        "    return {\"final_answer\": final_response}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
